{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%config InlineBackend.figure_format = 'retina'\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV, KFold\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import make_scorer, accuracy_score, f1_score, roc_auc_score\n",
    "from sklearn.model_selection import GridSearchCV, StratifiedKFold\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "\n",
    "np.random.seed(42)\n",
    "\n",
    "def encodeBind(df, features):\n",
    "    dummies = pd.get_dummies(df[features])\n",
    "    resDF = pd.concat([df, dummies], axis=1)\n",
    "    resDF.drop(features, axis=1, inplace = True)\n",
    "    return(resDF)\n",
    "\n",
    "def preprocessDF(df, featureEncode, featureScale):\n",
    "    scaler = StandardScaler()\n",
    "    dfLabel = df.goal\n",
    "\n",
    "    df.drop(\"goal\", axis = 1, inplace = True)\n",
    "    if featureEncode != None:\n",
    "        df = encodeBind(df, featureEncode)\n",
    "    df[\"goal\"] = dfLabel\n",
    "\n",
    "    df[featureScale] = scaler.fit_transform(df[featureScale])\n",
    "    \n",
    "    return(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adult dataset\n",
    "adultNames = [\"age\", \"workclass\", \"fnlwgt\", \"education\", \"education-num\", \"marital-status\", \n",
    "              \"occupation\", \"relationship\", \"race\", \"sex\", \"capital-gain\", \"capital-loss\", \n",
    "              \"hours-per-week\", \"native-country\", \"goal\"]\n",
    "adultEncode = [\"workclass\", \"education\", \"marital-status\", \"occupation\", \"relationship\", \"race\", \"sex\", \"native-country\"]\n",
    "adultScale = [\"age\", \"fnlwgt\", \"education-num\", \"capital-gain\", \"capital-loss\", \"hours-per-week\"]\n",
    "\n",
    "adult = pd.read_table(\"adult.data\", names = adultNames, sep = \",\\s\", engine='python')\n",
    "adult = adult[adult[\"workclass\"] != \"?\"]\n",
    "adult[\"goal\"] = np.where(adult.goal == \">50K\", 1, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "adult[adultScale].hist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "adult = preprocessDF(adult, adultEncode, adultScale)\n",
    "adult"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Covertype dataset\n",
    "covNames = [\"elevation\", \"aspect\", \"slope\", \"hordishydro\", \"verdishydro\", \"hordisroad\", \"hillam\", \"hillnoon\", \"hillpm\", \n",
    "            \"hordisfire\"] + [\"wild\" + str(i) for i in range(1,5)] + [\"soil\" + str(i) for i in range(1,41)] + [\"goal\"]\n",
    "covScale = [\"elevation\", \"aspect\", \"slope\", \"hordishydro\", \"verdishydro\", \"hordisroad\", \"hillam\", \"hillnoon\", \"hillpm\", \n",
    "            \"hordisfire\"]\n",
    "cov = pd.read_table(\"covtype.data\", sep = \",\", names = covNames)\n",
    "cov.goal = np.where(cov.goal == cov[\"goal\"].value_counts().idxmax(), 1, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cov[covScale].hist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cov = preprocessDF(cov, None, covScale)\n",
    "cov"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Letter dataset\n",
    "letterNames = [\"goal\", \"x-box\", \"y-box\", \"width\", \"height\", \"onpix\", \"x-bar\",\n",
    "              \"y-bar\", \"x2bar\", \"y2bar\", \"xybar\", \"x2ybr\", \"xy2br\", \"x-ege\",\n",
    "              \"xegvy\", \"y-ege\", \"yegvx\"]\n",
    "letterScale = [\"x-box\", \"y-box\", \"width\", \"height\", \"onpix\", \"x-bar\",\n",
    "              \"y-bar\", \"x2bar\", \"y2bar\", \"xybar\", \"x2ybr\", \"xy2br\", \"x-ege\",\n",
    "              \"xegvy\", \"y-ege\", \"yegvx\"]\n",
    "letter = pd.read_table(\"letter-recognition.data\", sep = \",\", names = letterNames)\n",
    "letterCols = list(letter.columns)\n",
    "letterCols[-1], letterCols[0] = letterCols[0], letterCols[-1]\n",
    "letter = letter[letterCols]\n",
    "chosenLetter = [chr(i) for i in range(ord('A'), ord('M')+1)]\n",
    "letter.goal = letter.goal.apply(lambda x: 1 if x in chosenLetter else 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "letter[letterScale].hist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "letter = preprocessDF(letter, None, letterScale)\n",
    "letter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Avila dataset\n",
    "avilaNames = [\"interdis\", \"upmar\", \"lowmar\", \"exploi\", \"rownum\", \"modratio\",\n",
    "             \"interspace\", \"weight\", \"peaknum\", \"modratio-interspace\", \"goal\"]\n",
    "avila = pd.read_csv(\"avila.txt\", sep = \",\", names = avilaNames)\n",
    "avila.goal = np.where(avila.goal == \"A\", 1, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "avila[list(avila.columns)[:-1]].hist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "avila"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bean dataset\n",
    "#beanNames = [\"\"]\n",
    "bean = pd.read_excel(\"Dry_Bean_Dataset.xlsx\", engine='openpyxl')\n",
    "beanScale = list(bean.columns)[:-1]\n",
    "bean.Class = np.where(bean.Class == bean.Class.value_counts().idxmax(), 1, 0)\n",
    "bean.columns = [*bean.columns[:-1], 'goal']\n",
    "bean[beanScale].hist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bean = preprocessDF(bean, None, beanScale)\n",
    "bean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getTrialTrainTest(pipe, param, score_function, X_train, X_test, Y_train, Y_test):\n",
    "    pipe.set_params(**param)\n",
    "    pipe.fit(X_train, Y_train)\n",
    "    y_pred_train = pipe.predict(X_train)\n",
    "    y_pred_test = pipe.predict(X_test)\n",
    "    trialTrain = score_function(Y_train, y_pred_train)\n",
    "    trialTest = score_function(Y_test, y_pred_test)\n",
    "    \n",
    "    return trialTrain, trialTest\n",
    "\n",
    "def spitoutres(data, pipe, param):\n",
    "    aucTrialTrain = []\n",
    "    accTrialTrain = []\n",
    "    f1TrialTrain = []\n",
    "    aucTrialTest = []\n",
    "    accTrialTest = []\n",
    "    f1TrialTest = []\n",
    "    for trial in range(5):\n",
    "        print(trial)\n",
    "        X, Y = data.iloc[:,:-1], data.iloc[:,-1]\n",
    "        X_train, X_test, Y_train, Y_test = train_test_split(X, Y, train_size=5000, shuffle=True)\n",
    "        gs = GridSearchCV(pipe, param_grid = param, cv = StratifiedKFold(), n_jobs = -1, \n",
    "                          scoring = scoring, refit = False)\n",
    "        gs.fit(X_train, Y_train)\n",
    "        results = gs.cv_results_\n",
    "    \n",
    "        auc_best_index = np.argmin(results['rank_test_AUC'])\n",
    "        auc_best_param = results['params'][auc_best_index]\n",
    "    \n",
    "        accuracy_best_index = np.argmin(results['rank_test_Accuracy'])\n",
    "        accuracy_best_param = results['params'][accuracy_best_index]\n",
    "    \n",
    "        f1_best_index = np.argmin(results['rank_test_F1'])\n",
    "        f1_best_param = results['params'][f1_best_index]\n",
    "    \n",
    "        aucTr, aucTest = getTrialTrainTest(pipe, auc_best_param, roc_auc_score, X_train, X_test, Y_train, Y_test)\n",
    "        accTr, accTest = getTrialTrainTest(pipe, accuracy_best_param, accuracy_score, X_train, X_test, Y_train, Y_test)\n",
    "        f1Tr, f1Test = getTrialTrainTest(pipe, f1_best_param, f1_score, X_train, X_test, Y_train, Y_test)\n",
    "        \n",
    "        aucTrialTrain.append(aucTr)\n",
    "        accTrialTrain.append(accTr)\n",
    "        f1TrialTrain.append(f1Tr)\n",
    "        aucTrialTest.append(aucTest)\n",
    "        accTrialTest.append(accTest)\n",
    "        f1TrialTest.append(f1Test)\n",
    "        \n",
    "    return aucTrialTrain, accTrialTrain, f1TrialTrain, aucTrialTest, accTrialTest, f1TrialTest\n",
    "\n",
    "def runPerAlgo(algo, datasets, param):\n",
    "    aucTrainMean, accTrainMean, f1TrainMean, aucTestMean, accTestMean, f1TestMean = [], [], [], [], [], []\n",
    "    aucTrainRaw, accTrainRaw, f1TrainRaw, aucTestRaw, accTestRaw, f1TestRaw = [], [], [], [], [], []\n",
    "    for data in datasets:\n",
    "        aucTrain, accTrain, f1Train, aucTest, accTest, f1Test = spitoutres(data, algo, param)\n",
    "\n",
    "        accTrainRaw.append(accTrain)\n",
    "        accTestRaw.append(accTest)\n",
    "        \n",
    "        aucTrainMean.append(np.mean(aucTrain))\n",
    "        accTrainMean.append(np.mean(accTrain))\n",
    "        f1TrainMean.append(np.mean(f1Train))\n",
    "        aucTestMean.append(np.mean(aucTest))\n",
    "        accTestMean.append(np.mean(accTest))\n",
    "        f1TestMean.append(np.mean(f1Test))\n",
    "        \n",
    "    print(\"Raw train score\")\n",
    "\n",
    "    accTrainDF = pd.DataFrame(accTrainRaw, columns = [\"Trial1\", \"Trial2\", \"Trial3\", \"Trial 4\", \"Trial 5\"], dtype = float)\n",
    "    print(accTrainDF)\n",
    "    \n",
    "    print(\"Raw test score\")\n",
    "\n",
    "    accTestDF = pd.DataFrame(accTestRaw, columns = [\"Trial1\", \"Trial2\", \"Trial3\", \"Trial 4\", \"Trial 5\"], dtype = float)\n",
    "    print(accTestDF)\n",
    "    \n",
    "    theDFTest = pd.DataFrame(list(zip(aucTestMean, accTestMean, f1TestMean)), columns = [\"AUC\", \"ACC\", \"F1\"], dtype = float)\n",
    "    theDFTrain = pd.DataFrame(list(zip(aucTrainMean, accTrainMean, f1TrainMean)), columns = [\"AUC\", \"ACC\", \"F1\"], dtype = float)\n",
    "    \n",
    "    print(\"Test over problems\")\n",
    "    print(theDFTest.mean(axis = 1))\n",
    "    print(\"Test over metrics\")\n",
    "    print(theDFTest.mean(axis = 0))\n",
    "    \n",
    "    print(\"Train over problems\")\n",
    "    print(theDFTrain.mean(axis = 1))\n",
    "    print(\"Train over metrics\")\n",
    "    print(theDFTrain.mean(axis = 0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scoring = {'AUC': 'roc_auc', 'Accuracy': make_scorer(accuracy_score), 'F1': make_scorer(f1_score)}\n",
    "\n",
    "knnParam = {'knn__n_neighbors': np.arange(1,106,4),\n",
    "    'knn__weights': [\"uniform\", \"distance\"]}\n",
    "nnParam = [{\n",
    "    \"nn__hidden_layer_sizes\": [(1,), (2,), (4,), (8,), (32,), (128,)],\n",
    "    \"nn__solver\": [\"adam\"]\n",
    "}, {\n",
    "    \"nn__hidden_layer_sizes\": [(1,), (2,), (4,), (8,), (32,), (128,)],\n",
    "    \"nn__solver\": [\"sgd\"],\n",
    "    \"nn__momentum\": [0, 0.2, 0.5, 0.9]\n",
    "}]\n",
    "rfParam = {'rf__max_features': [\"auto\", \"log2\", None, 1, 2, 4, 6, 8, 10]}\n",
    "lrParam = [{\n",
    "    'lr__solver': ['saga'],\n",
    "    'lr__penalty': ['l1', 'l2'],\n",
    "    'lr__C': [10**i for i in range(-8,5)]\n",
    "    }, {\n",
    "    'lr__solver': ['lbfgs'],\n",
    "    'lr__penalty': ['l2'],\n",
    "    'lr__C': [10**i for i in range(-8,5)]\n",
    "    }, {\n",
    "    'lr__solver': ['lbfgs','saga'],\n",
    "    'lr__penalty': ['none'],\n",
    "}]\n",
    "\n",
    "knnPipe = Pipeline([(\"knn\", KNeighborsClassifier())])\n",
    "nnPipe = Pipeline([(\"nn\", MLPClassifier(max_iter=500))])\n",
    "rfPipe = Pipeline([(\"rf\", RandomForestClassifier(n_estimators = 1024, n_jobs = -1))])\n",
    "lrPipe = Pipeline([(\"lr\", LogisticRegression(max_iter = 5000))])\n",
    "\n",
    "params = [knnParam, nnParam, rfParam, lrParam]\n",
    "pipes = [knnPipe, nnPipe, rfPipe, lrPipe]\n",
    "datasets = [adult, cov, letter, avila, bean]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### KNN model\n",
    "runPerAlgo(knnPipe, datasets, knnParam)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### NN model\n",
    "runPerAlgo(nnPipe, datasets, nnParam)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### RF Model\n",
    "runPerAlgo(rfPipe, datasets, rfParam)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### LR Model\n",
    "runPerAlgo(lrPipe, datasets, lrParam)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
