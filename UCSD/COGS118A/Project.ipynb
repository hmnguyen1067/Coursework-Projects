{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Importing packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%config InlineBackend.figure_format = 'retina'\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV, KFold\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import make_scorer, accuracy_score, f1_score, roc_auc_score\n",
    "from sklearn.model_selection import GridSearchCV, StratifiedKFold\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from typing import Optional\n",
    "%matplotlib inline\n",
    "\n",
    "\n",
    "np.random.seed(42)\n",
    "\n",
    "DIRECTORY = \"./data/\"\n",
    "\n",
    "def encodeBind(df: pd.DataFrame, features: list) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    One-hot encoding the categorical variables in dataframe\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    df: pd.DataFrame\n",
    "    The dataframe that need to be encoded.\n",
    "\n",
    "    features: list\n",
    "    A list of categorical variables that need to be encoded.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    A one-hot encoded dataframe.\n",
    "    \"\"\"\n",
    "    dummies = pd.get_dummies(df[features])\n",
    "    resDF = pd.concat([df, dummies], axis=1)\n",
    "    resDF.drop(features, axis=1, inplace = True)\n",
    "    return resDF\n",
    "\n",
    "def preprocessDF(df: pd.DataFrame, \n",
    "            featureScale:list, \n",
    "            featureEncode: Optional[list] = None) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    One-hot encoding the categorical variables in dataframe.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    df: pd.DataFrame\n",
    "    The dataframe that need to be preprocessed.\n",
    "\n",
    "    featureScale: list\n",
    "    A list of quantitative variables that need to be scaled.\n",
    "\n",
    "    featureEncode: list, optional\n",
    "    A list of categorical variables that need to be encoded.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    A preprocessed dataframe with quantitative variables\n",
    "    scaled and categorical variables one-hot encoded.\n",
    "    \"\"\"\n",
    "    scaler = StandardScaler()\n",
    "    dfLabel = df.goal\n",
    "\n",
    "    df.drop(\"goal\", axis = 1, inplace = True)\n",
    "    if featureEncode != None:\n",
    "        df = encodeBind(df, featureEncode)\n",
    "    df[\"goal\"] = dfLabel\n",
    "\n",
    "    df[featureScale] = scaler.fit_transform(df[featureScale])\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data clean-up"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Adult dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "adultNames = [\"age\", \"workclass\", \"fnlwgt\", \"education\", \"education-num\", \"marital-status\", \n",
    "              \"occupation\", \"relationship\", \"race\", \"sex\", \"capital-gain\", \"capital-loss\", \n",
    "              \"hours-per-week\", \"native-country\", \"goal\"]\n",
    "adultEncode = [\"workclass\", \"education\", \"marital-status\", \"occupation\", \"relationship\", \"race\", \"sex\", \"native-country\"]\n",
    "adultScale = [\"age\", \"fnlwgt\", \"education-num\", \"capital-gain\", \"capital-loss\", \"hours-per-week\"]\n",
    "\n",
    "adult = pd.read_table(\n",
    "        os.path.join(DIRECTORY +\"adult.data\"),\n",
    "        names = adultNames,\n",
    "        sep = \",\\s\", \n",
    "        engine='python')\n",
    "adult = adult[adult[\"workclass\"] != \"?\"]\n",
    "adult[\"goal\"] = np.where(adult.goal == \">50K\", 1, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "adultScatterMat = pd.DataFrame(adult[adultScale], \n",
    "        columns = adultScale)\n",
    "adultScatterMat[\"target\"] = adult[\"goal\"].values\n",
    "\n",
    "sns.pairplot(adultScatterMat, hue = \"target\", diag_kind=\"hist\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "adult = preprocessDF(adult, adultScale, adultEncode)\n",
    "adult.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Covertype dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "covNames = [\"elevation\", \"aspect\", \"slope\", \"hordishydro\", \"verdishydro\", \"hordisroad\", \"hillam\", \"hillnoon\", \"hillpm\", \n",
    "            \"hordisfire\"] + [\"wild\" + str(i) for i in range(1,5)] + [\"soil\" + str(i) for i in range(1,41)] + [\"goal\"]\n",
    "covScale = [\"elevation\", \"aspect\", \"slope\", \"hordishydro\", \"verdishydro\", \"hordisroad\", \"hillam\", \"hillnoon\", \"hillpm\", \n",
    "            \"hordisfire\"]\n",
    "\n",
    "cov = pd.read_table(\n",
    "        os.path.join(DIRECTORY +\"covtype.data\"), \n",
    "        sep = \",\", \n",
    "        names = covNames)\n",
    "cov.goal = np.where(cov.goal == cov[\"goal\"].value_counts().idxmax(), 1, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "covScatterMat = pd.DataFrame(cov[covScale], \n",
    "        columns = covScale)\n",
    "covScatterMat[\"target\"] = cov[\"goal\"].values\n",
    "\n",
    "sns.pairplot(covScatterMat, hue = \"target\", diag_kind=\"hist\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cov = preprocessDF(cov, covScale, None)\n",
    "cov.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Letter dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "letterNames = [\"goal\", \"x-box\", \"y-box\", \"width\", \"height\", \"onpix\", \"x-bar\",\n",
    "              \"y-bar\", \"x2bar\", \"y2bar\", \"xybar\", \"x2ybr\", \"xy2br\", \"x-ege\",\n",
    "              \"xegvy\", \"y-ege\", \"yegvx\"]\n",
    "letterScale = [\"x-box\", \"y-box\", \"width\", \"height\", \"onpix\", \"x-bar\",\n",
    "              \"y-bar\", \"x2bar\", \"y2bar\", \"xybar\", \"x2ybr\", \"xy2br\", \"x-ege\",\n",
    "              \"xegvy\", \"y-ege\", \"yegvx\"]\n",
    "\n",
    "letter = pd.read_table(\n",
    "        os.path.join(DIRECTORY + \"letter-recognition.data\"), \n",
    "        sep = \",\", \n",
    "        names = letterNames)\n",
    "letterCols = list(letter.columns)\n",
    "letterCols[-1], letterCols[0] = letterCols[0], letterCols[-1]\n",
    "letter = letter[letterCols]\n",
    "chosenLetter = [chr(i) for i in range(ord('A'), ord('M')+1)]\n",
    "letter.goal = letter.goal.apply(lambda x: 1 if x in chosenLetter else 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "letterScatterMat = pd.DataFrame(letter[letterScale], \n",
    "        columns = letterScale)\n",
    "letterScatterMat[\"target\"] = letter[\"goal\"].values\n",
    "\n",
    "sns.pairplot(letterScatterMat, hue = \"target\", diag_kind=\"hist\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "letter = preprocessDF(letter, letterScale, None)\n",
    "letter.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Avila dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "avilaNames = [\"interdis\", \"upmar\", \"lowmar\", \"exploi\", \"rownum\", \"modratio\",\n",
    "             \"interspace\", \"weight\", \"peaknum\", \"modratio-interspace\", \"goal\"]\n",
    "\n",
    "avila = pd.read_csv(os.path.join(DIRECTORY +\"avila.txt\"), \n",
    "        sep = \",\", \n",
    "        names = avilaNames)\n",
    "avilaScale = list(avila.columns)[:-1]\n",
    "avila.goal = np.where(avila.goal == \"A\", 1, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "avilaScatterMat = pd.DataFrame(avila[avilaScale], \n",
    "        columns = avilaScale)\n",
    "avilaScatterMat[\"target\"] = avila[\"goal\"].values\n",
    "\n",
    "sns.pairplot(avilaScatterMat, hue = \"target\", diag_kind=\"hist\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "avila.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bean dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bean = pd.read_excel(os.path.join(DIRECTORY +\"Dry_Bean_Dataset.xlsx\"), \n",
    "        engine='openpyxl')\n",
    "        \n",
    "beanScale = list(bean.columns)[:-1]\n",
    "bean.Class = np.where(bean.Class == bean.Class.value_counts().idxmax(), 1, 0)\n",
    "bean.columns = [*bean.columns[:-1], 'goal']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "beanScatterMat = pd.DataFrame(bean[beanScale], \n",
    "        columns = beanScale)\n",
    "beanScatterMat[\"target\"] = bean[\"goal\"].values\n",
    "\n",
    "sns.pairplot(beanScatterMat, hue = \"target\", diag_kind=\"hist\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bean = preprocessDF(bean, beanScale)\n",
    "bean.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Function definition for main program"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_trail_train_test(pipe: Pipeline, \n",
    "                param: dict, \n",
    "                score_function, \n",
    "                X_train, \n",
    "                X_test, \n",
    "                Y_train, \n",
    "                Y_test):\n",
    "    \"\"\"\n",
    "    Given a pipeline and the parameters, the function will give the \n",
    "    metric score of the training and testing prediction.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    pipe: Pipeline\n",
    "    An algorithm model pipeline.\n",
    "\n",
    "    param: dict\n",
    "    The parameters the algorithm used for grid searching.\n",
    "\n",
    "    score_function: sklearn.metrics\n",
    "    A metric function to give a score.\n",
    "\n",
    "    X_train, X_test, Y_train, Y_test: np.ndarray\n",
    "    Training and testing data with labels already splitted.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    Return the score of the metrics given for the training and testing set.\n",
    "    \"\"\"                \n",
    "    pipe.set_params(**param)\n",
    "    pipe.fit(X_train, Y_train)\n",
    "    y_pred_train = pipe.predict(X_train)\n",
    "    y_pred_test = pipe.predict(X_test)\n",
    "    trialTrain = score_function(Y_train, y_pred_train)\n",
    "    trialTest = score_function(Y_test, y_pred_test)\n",
    "    \n",
    "    return trialTrain, trialTest\n",
    "\n",
    "def spit_out_res(data: pd.DataFrame, \n",
    "                pipe: Pipeline, \n",
    "                param: dict):\n",
    "    \"\"\"\n",
    "    The function will compute the metrics (in this case, AUC, \n",
    "    accuracy and F1-score) from grid search cross-validation.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    data: pd.DataFrame\n",
    "    Training and testing data with labels in a dataframe.\n",
    "\n",
    "    pipe: Pipeline\n",
    "    An algorithm model pipeline.\n",
    "\n",
    "    param: dict\n",
    "    The parameters the algorithm used for grid searching.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    Return the AUC, accuracy and F1-score of the given data \n",
    "    with a specific algorithm and parameter. \n",
    "    \"\"\"\n",
    "    aucTrialTrain = []\n",
    "    accTrialTrain = []\n",
    "    f1TrialTrain = []\n",
    "    aucTrialTest = []\n",
    "    accTrialTest = []\n",
    "    f1TrialTest = []\n",
    "    for trial in range(5):\n",
    "        print(trial)\n",
    "        X, Y = data.iloc[:,:-1], data.iloc[:,-1]\n",
    "        X_train, X_test, Y_train, Y_test = train_test_split(X, Y, train_size=5000, shuffle=True)\n",
    "        gs = GridSearchCV(pipe, param_grid = param, cv = StratifiedKFold(), n_jobs = -1, \n",
    "                          scoring = scoring, refit = False)\n",
    "        gs.fit(X_train, Y_train)\n",
    "        results = gs.cv_results_\n",
    "    \n",
    "        auc_best_index = np.argmin(results['rank_test_AUC'])\n",
    "        auc_best_param = results['params'][auc_best_index]\n",
    "    \n",
    "        accuracy_best_index = np.argmin(results['rank_test_Accuracy'])\n",
    "        accuracy_best_param = results['params'][accuracy_best_index]\n",
    "    \n",
    "        f1_best_index = np.argmin(results['rank_test_F1'])\n",
    "        f1_best_param = results['params'][f1_best_index]\n",
    "    \n",
    "        aucTr, aucTest = get_trail_train_test(pipe, auc_best_param, roc_auc_score, X_train, X_test, Y_train, Y_test)\n",
    "        accTr, accTest = get_trail_train_test(pipe, accuracy_best_param, accuracy_score, X_train, X_test, Y_train, Y_test)\n",
    "        f1Tr, f1Test = get_trail_train_test(pipe, f1_best_param, f1_score, X_train, X_test, Y_train, Y_test)\n",
    "        \n",
    "        aucTrialTrain.append(aucTr)\n",
    "        accTrialTrain.append(accTr)\n",
    "        f1TrialTrain.append(f1Tr)\n",
    "        aucTrialTest.append(aucTest)\n",
    "        accTrialTest.append(accTest)\n",
    "        f1TrialTest.append(f1Test)\n",
    "        \n",
    "    return aucTrialTrain, accTrialTrain, f1TrialTrain, aucTrialTest, accTrialTest, f1TrialTest\n",
    "\n",
    "def run_per_algo(algoPipe: Pipeline, \n",
    "                datasets: list, \n",
    "                param: dict):\n",
    "    \"\"\"\n",
    "    The function will calculate the raw and mean test set performance across \n",
    "    trails for an algorithm with all datasets over three metrics. Furthermore,\n",
    "    it also includes the training set performance.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    algoPipe: Pipeline\n",
    "    An algorithm model pipeline.\n",
    "\n",
    "    datasets: list\n",
    "    A list of datasets which we will iterate over.\n",
    "\n",
    "    param: dict\n",
    "    The parameters the algorithm used for grid searching.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    Return 6 tables for raw and mean training and testing score over 3 metrics.\n",
    "    \"\"\"\n",
    "    aucTrainMean, accTrainMean, f1TrainMean, aucTestMean, accTestMean, f1TestMean = [], [], [], [], [], []\n",
    "    aucTrainRaw, accTrainRaw, f1TrainRaw, aucTestRaw, accTestRaw, f1TestRaw = [], [], [], [], [], []\n",
    "    for data in datasets:\n",
    "        aucTrain, accTrain, f1Train, aucTest, accTest, f1Test = spit_out_res(data, algoPipe, param)\n",
    "\n",
    "        accTrainRaw.append(accTrain)\n",
    "        accTestRaw.append(accTest)\n",
    "        \n",
    "        aucTrainMean.append(np.mean(aucTrain))\n",
    "        accTrainMean.append(np.mean(accTrain))\n",
    "        f1TrainMean.append(np.mean(f1Train))\n",
    "        aucTestMean.append(np.mean(aucTest))\n",
    "        accTestMean.append(np.mean(accTest))\n",
    "        f1TestMean.append(np.mean(f1Test))\n",
    "        \n",
    "    print(\"Raw train score\")\n",
    "\n",
    "    accTrainDF = pd.DataFrame(accTrainRaw, columns = [\"Trial1\", \"Trial2\", \"Trial3\", \"Trial 4\", \"Trial 5\"], dtype = float)\n",
    "    print(accTrainDF)\n",
    "    \n",
    "    print(\"Raw test score\")\n",
    "\n",
    "    accTestDF = pd.DataFrame(accTestRaw, columns = [\"Trial1\", \"Trial2\", \"Trial3\", \"Trial 4\", \"Trial 5\"], dtype = float)\n",
    "    print(accTestDF)\n",
    "    \n",
    "    theDFTest = pd.DataFrame(list(zip(aucTestMean, accTestMean, f1TestMean)), columns = [\"AUC\", \"ACC\", \"F1\"], dtype = float)\n",
    "    theDFTrain = pd.DataFrame(list(zip(aucTrainMean, accTrainMean, f1TrainMean)), columns = [\"AUC\", \"ACC\", \"F1\"], dtype = float)\n",
    "    \n",
    "    print(\"Test over problems\")\n",
    "    print(theDFTest.mean(axis = 1))\n",
    "    print(\"Test over metrics\")\n",
    "    print(theDFTest.mean(axis = 0))\n",
    "    \n",
    "    print(\"Train over problems\")\n",
    "    print(theDFTrain.mean(axis = 1))\n",
    "    print(\"Train over metrics\")\n",
    "    print(theDFTrain.mean(axis = 0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Parameters for grid search cross-validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scoring = {'AUC': 'roc_auc', 'Accuracy': make_scorer(accuracy_score), 'F1': make_scorer(f1_score)}\n",
    "\n",
    "knnParam = {'knn__n_neighbors': np.arange(1,106,4),\n",
    "    'knn__weights': [\"uniform\", \"distance\"]}\n",
    "\n",
    "nnParam = [{\n",
    "    \"nn__hidden_layer_sizes\": [(1,), (2,), (4,), (8,), (32,), (128,)],\n",
    "    \"nn__solver\": [\"adam\"]\n",
    "}, {\n",
    "    \"nn__hidden_layer_sizes\": [(1,), (2,), (4,), (8,), (32,), (128,)],\n",
    "    \"nn__solver\": [\"sgd\"],\n",
    "    \"nn__momentum\": [0, 0.2, 0.5, 0.9]\n",
    "}]\n",
    "\n",
    "rfParam = {\n",
    "    'rf__max_features': [\"auto\", \"log2\", None, 1, 2, 4, 6, 8, 10]\n",
    "    }\n",
    "\n",
    "lrParam = [{\n",
    "    'lr__solver': ['saga'],\n",
    "    'lr__penalty': ['l1', 'l2'],\n",
    "    'lr__C': [10**i for i in range(-8,5)]\n",
    "    }, {\n",
    "    'lr__solver': ['lbfgs'],\n",
    "    'lr__penalty': ['l2'],\n",
    "    'lr__C': [10**i for i in range(-8,5)]\n",
    "    }, {\n",
    "    'lr__solver': ['lbfgs','saga'],\n",
    "    'lr__penalty': ['none'],\n",
    "}]\n",
    "\n",
    "knnPipe = Pipeline([(\"knn\", KNeighborsClassifier())])\n",
    "nnPipe = Pipeline([(\"nn\", MLPClassifier(max_iter=500))])\n",
    "rfPipe = Pipeline([(\"rf\", RandomForestClassifier(n_estimators = 1024, n_jobs = -1))])\n",
    "lrPipe = Pipeline([(\"lr\", LogisticRegression(max_iter = 5000))])\n",
    "\n",
    "params = [knnParam, nnParam, rfParam, lrParam]\n",
    "pipes = [knnPipe, nnPipe, rfPipe, lrPipe]\n",
    "datasets = [adult, cov, letter, avila, bean]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Program run"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## K nearest neighbors model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run_per_algo(knnPipe, datasets, knnParam)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MLP model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run_per_algo(nnPipe, datasets, nnParam)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Forest Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run_per_algo(rfPipe, datasets, rfParam)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic Regression Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run_per_algo(lrPipe, datasets, lrParam)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
